{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjGjeLNiYqfcJmwXYyyaHy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chetirebochki/PythonLibruary/blob/main/12glava.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4qdu7DEpkXm"
      },
      "outputs": [],
      "source": [
        "#выводит на экран строку отзыва и его метку класса (positive или negative).\n",
        "def pretty_print_review_and_label(i):\n",
        "   print(labels[i] + \"\\t:\\t\" + reviews[i][:80] + \"...\")\n",
        "\n",
        "#Открываем файл с отзывами и записываем их построчно в список reviews, \n",
        "#используя map, чтобы удалить символ переноса строки в конце каждой строки.\n",
        "g = open('reviews.txt','r') # What we know!\n",
        "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
        "\n",
        "#Закрываем файл с отзывами.\n",
        "g.close()\n",
        "\n",
        "#Открываем файл с метками классов и записываем их построчно в список labels, \n",
        "g = open('labels.txt','r') # What we WANT to know!\n",
        "\n",
        "#используя map, чтобы перевести все буквы в верхний регистр и удалить символ переноса строки в конце каждой строки.\n",
        "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
        "\n",
        "#Закрываем файл с метками классов.\n",
        "g.close()\n",
        "\n",
        "\n",
        "# Preprocess dataset:\n",
        "\n",
        "import sys\n",
        "# Открываем файл с отзывами в переменной f и записываем их в список raw_reviews.\n",
        "f = open('reviews.txt')\n",
        "raw_reviews = f.readlines()\n",
        "f.close()\n",
        "\n",
        "#Открываем файл с метками классов в переменной f и записываем их в список raw_labels.\n",
        "f = open('labels.txt')\n",
        "raw_labels = f.readlines()\n",
        "f.close()\n",
        "\n",
        "#Разбиваем каждый отзыв на множество уникальных слов (tokens)\n",
        "# и записываем результат в список tokens, используя lambda-функцию в map для удаления символов переноса строки в конце каждой строки.\n",
        "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
        "\n",
        "#Создаем пустой набор слов vocab и используем его для записи всех уникальных слов из всех отзывов в tokens.\n",
        "vocab = set()\n",
        "\n",
        "#Конвертируем vocab в список.\n",
        "for sent in tokens:\n",
        "    for word in sent:\n",
        "        if(len(word)>0):\n",
        "            vocab.add(word)\n",
        "vocab = list(vocab)\n",
        "\n",
        "#Создаем словарь word2index, в котором каждому слову из vocab присваивается уникальный индекс.\n",
        "word2index = {}\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "\n",
        "#Создаем пустой список input_dataset для записи индексов слов в каждом отзыве.\n",
        "input_dataset = list()\n",
        "\n",
        "#Проходим через каждый отзыв в списке tokens и каждое слово внутри отзыва переводим в его индекс из словаря word2index,\n",
        "# если слово содержится в словаре. Индексы слов в отзыве добавляем в список sent_indices. \n",
        "#Когда все слова из отзыва переведены в индексы, добавляем список sent_indices в input_dataset.\n",
        "for sent in tokens:\n",
        "    sent_indices = list()\n",
        "    for word in sent:\n",
        "        try:\n",
        "            sent_indices.append(word2index[word])\n",
        "        except:\n",
        "            \"\"\n",
        "    input_dataset.append(list(set(sent_indices)))\n",
        "\n",
        "#Создаем пустой список target_dataset для записи меток классов (1 - если отзыв положительный, 0 - если отзыв отрицательный).\n",
        "target_dataset = list()\n",
        "\n",
        "#Проходим через каждую метку класса в списке raw_labels и добавляем метку 1 в список target_dataset,\n",
        "# если отзыв положительный (label == 'positive\\n'), и метку 0, если отрицательный.\n",
        "for label in raw_labels:\n",
        "    if label == 'positive\\n':\n",
        "        target_dataset.append(1)\n",
        "    else:\n",
        "        target_dataset.append(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "#собираем параметры из главы 11\n",
        "alpha, iterations = (0.05, 2)\n",
        "hidden_size,window,negative = (50,2,5)\n",
        "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
        "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0\n",
        "\n",
        "#Умножаем матрицу весов weights01 саму на себя, \n",
        "#чтобы получить сумму квадратов весов для каждого слова из словаря. Результат записываем в массив norms.\n",
        "norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
        "\n",
        "#Изменяем размерности массива norms до размера (количество слов в словаре, 1).\n",
        "norms.resize(norms.shape[0],1)\n",
        "\n",
        "#Нормируем веса weights01 на основе полученных квадратных норм векторов и записываем результат в массив normedweights.\n",
        "normed_weights = weights_0_1 * norms\n",
        "\n",
        "#Создаем функцию makesentvect(words), \n",
        "#которая получает на вход список слов и возвращает средний вектор взвешенных индексов слов в этом списке.\n",
        "def make_sent_vect(words):\n",
        "    indices = list(map(lambda x:word2index[x],filter(lambda x:x in word2index,words)))\n",
        "    return np.mean(normed_weights[indices],axis=0)\n",
        "\n",
        "#Инициализируем пустой список reviews2vectors.\n",
        "reviews2vectors = list()\n",
        "\n",
        "#Проходим через каждый отзыв в списке tokens \n",
        "#и добавляем в reviews2vectors средний вектор взвешенных индексов слов в этом отзыве, \n",
        "#построенный с помощью функции makesentvect.\n",
        "for review in tokens: # tokenized reviews\n",
        "    reviews2vectors.append(make_sent_vect(review))\n",
        "reviews2vectors = np.array(reviews2vectors)\n",
        "\n",
        "#Создаем функцию mostsimilarreviews(review), с помощью скалярного произведения векторов. \n",
        "#Для этого сначала получаем средний вектор взвешенных индексов слов review с помощью makesentvect, \n",
        "#затем проходим через каждый отзыв в reviews2vectors и считаем скалярное произведение его вектора со средним вектором review. \n",
        "#Результат записываем в словарь scores, где ключ - индекс отзыва, значение - скалярное произведение оценки отзыва со значением вектора review. \n",
        "#Далее из словаря scores выбираем три элемента с самыми большими значениями методом mostcommon() \n",
        "#и записываем их индексы в список mostsimilar. Затем возвращаем 40 первых символов каждого отзыва в mostsimilar.\n",
        "def most_similar_reviews(review):\n",
        "    v = make_sent_vect(review)\n",
        "    scores = Counter()\n",
        "    for i,val in enumerate(reviews2vectors.dot(v)):\n",
        "        scores[i] = val\n",
        "    most_similar = list()\n",
        "    \n",
        "    for idx,score in scores.most_common(3):\n",
        "        most_similar.append(raw_reviews[idx][0:40])\n",
        "    return most_similar\n",
        "# Вызываем функцию mostsimilarreviews('boring','awful') для поиска наиболее похожих отзывов на слова \"boring\" и \"awful\".\n",
        "most_similar_reviews(['boring','awful'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzv6p7f7uUXC",
        "outputId": "9f2b2a68-de52-4512-8da6-91d176787163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the characters are unlikeable and the sc',\n",
              " 'adrian pasdar is excellent is this film ',\n",
              " 'long  boring  blasphemous . never have i']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#создаем массивы \n",
        "a = np.array([1,2,3])\n",
        "b = np.array([0.1,0.2,0.3])\n",
        "c = np.array([-1,-0.5,0])\n",
        "d = np.array([0,0,0])\n",
        "\n",
        "#Создаем двумерный массив identity, представляющий собой единичную матрицу размера 3x3.\n",
        "identity = np.eye(3)\n",
        "print(identity)\n",
        "print(a.dot(identity))\n",
        "print(b.dot(identity))\n",
        "print(c.dot(identity))\n",
        "print(d.dot(identity))\n",
        "\n",
        "this = np.array([2,4,6])\n",
        "movie = np.array([10,10,10])\n",
        "rocks = np.array([1,1,1])\n",
        "#Выполняем поэлементное сложение массивов this, movie и rocks и выводим результат на экран.\n",
        "print(this + movie + rocks)\n",
        "#Выполняем матричное умножение массива this на identity, прибавляем к результату массив movie,\n",
        "# выполняем матричное умножение результата на identity  \n",
        "#и прибавляем массив rocks. Выводим полученный результат на экран.\n",
        "print((this.dot(identity) + movie).dot(identity) + rocks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h3X_8429W9T",
        "outputId": "c54765c9-e4fd-4fa3-938c-db2a9b2b3b44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[1. 2. 3.]\n",
            "[0.1 0.2 0.3]\n",
            "[-1.  -0.5  0. ]\n",
            "[0. 0. 0.]\n",
            "[13 15 17]\n",
            "[13. 15. 17.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#высчитываем вероятности для каждой строки матриццы\n",
        "def softmax(x_):\n",
        "    x = np.atleast_2d(x_)\n",
        "    temp = np.exp(x)\n",
        "    return temp / np.sum(temp, axis=1, keepdims=True)\n",
        "\n",
        "#инициализируется словарь word_vects с векторами для каждого слова.\n",
        "# Каждый вектор состоит из трех значений 0.\n",
        "word_vects = {}\n",
        "word_vects['yankees'] = np.array([[0.,0.,0.]])\n",
        "word_vects['bears'] = np.array([[0.,0.,0.]])\n",
        "word_vects['braves'] = np.array([[0.,0.,0.]])\n",
        "word_vects['red'] = np.array([[0.,0.,0.]])\n",
        "word_vects['socks'] = np.array([[0.,0.,0.]])\n",
        "word_vects['lose'] = np.array([[0.,0.,0.]])\n",
        "word_vects['defeat'] = np.array([[0.,0.,0.]])\n",
        "word_vects['beat'] = np.array([[0.,0.,0.]])\n",
        "word_vects['tie'] = np.array([[0.,0.,0.]])\n",
        "\n",
        "#sent2output является матрицей случайных чисел размером 3 на количество слов (здесь 9 слов)\n",
        "sent2output = np.random.rand(3,len(word_vects))\n",
        "\n",
        "#Затем определяются две матрицы identity и layer_0. \n",
        "#Layer_0 является вектором, соответствующим слову \"red\".\n",
        "identity = np.eye(3)\n",
        "layer_0 = word_vects['red']\n",
        "#Затем слои layer_1 и layer_2 вычисляются путем умножения векторов на identity \n",
        " #(единичную матрицу) и добавления векторов слов \"socks\" и \"defeat\" соответственно.\n",
        "layer_1 = layer_0.dot(identity) + word_vects['socks']\n",
        "layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
        "\n",
        "#Наконец, прогнозы получаются путем прохождения layer_2 через матрицу sent2output с последующим применением функции softmax. \n",
        "#Напечатанный результат представляет собой матрицу вероятностей для каждой строки матрицы pred.\n",
        "pred = softmax(layer_2.dot(sent2output))\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq7se2MnFQ5K",
        "outputId": "dad83cc0-2cad-4f58-efb8-a7d88a55154a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
            "  0.11111111 0.11111111 0.11111111]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как нам вернуться к этому?"
      ],
      "metadata": {
        "id": "vv747KIaeTy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Задаем сид для генератора псевдослучайных чисел библиотеки NumPy, \n",
        "#чтобы получить одинаковый результат каждый раз при запуске программы\n",
        "y = np.array([1,0,0,0,0,0,0,0,0]) # target one-hot vector for \"yankees\"\n",
        "\n",
        "## вычисляем разницу между предсказанием и целевым значением\n",
        "pred_delta = pred - y\n",
        "## вычисляем погрешность слоя скрытого состояния output\n",
        "layer_2_delta = pred_delta.dot(sent2output.T)\n",
        "\n",
        "defeat_delta = layer_2_delta * 1 # can ignore the \"1\" like prev. chapter\n",
        "\n",
        "# вычисляем погрешность слоя скрытого состояния hidden\n",
        "layer_1_delta = layer_2_delta.dot(identity.T)\n",
        "socks_delta = layer_1_delta * 1 # again... can ignore the \"1\"\n",
        "\n",
        "# вычисляем погрешность входного слоя input\n",
        "layer_0_delta = layer_1_delta.dot(identity.T)\n",
        "\n",
        " # коэффициент обучения\n",
        "alpha = 0.01\n",
        "\n",
        "\n",
        "# обновляем вектора \n",
        "word_vects['red'] -= layer_0_delta * alpha\n",
        "word_vects['socks'] -= socks_delta * alpha\n",
        "word_vects['defeat'] -= defeat_delta * alpha\n",
        "# обновляем матрицу весов между входным и скрытым слоями\n",
        "identity -= np.outer(layer_0,layer_1_delta) * alpha\n",
        "# обновляем матрицу весов между скрытым и выходным слоями\n",
        "identity -= np.outer(layer_1,layer_2_delta) * alpha\n",
        "# обновляем матрицу весов между скрытым слоем и выходом\n",
        "sent2output -= np.outer(layer_2,pred_delta) * alpha"
      ],
      "metadata": {
        "id": "KYoXagCoeVWY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнем обучение"
      ],
      "metadata": {
        "id": "wjVJ5HNYecmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys,random,math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "f = open('qa1_single-supporting-fact_train.txt','r')\n",
        "raw = f.readlines()\n",
        "f.close()\n",
        "\n",
        "tokens = list()\n",
        "for line in raw[0:1000]:\n",
        "  #добавление в список tokens нижнего регистра исходной строки без символов перевода строки и удалением первого слова в строке\n",
        "    tokens.append(line.lower().replace(\"\\n\",\"\").split(\" \")[1:])\n",
        "#вывод первых трех элементов списка tokens\n",
        "print(tokens[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHcYRP7je5si",
        "outputId": "9463e124-8ac5-4dc0-81bc-989174597fbc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['mary', 'moved', 'to', 'the', 'bathroom.'], ['john', 'went', 'to', 'the', 'hallway.'], ['where', 'is', 'mary?', '\\tbathroom\\t1']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#создание пустого множества\n",
        "vocab = set()\n",
        "for sent in tokens:#цикл для каждой строки в списке tokens\n",
        "    for word in sent:#цикл для каждой строки в списке tokens\n",
        "        vocab.add(word)#добавление слова в множество\n",
        "\n",
        "# перевод множества в список\n",
        "vocab = list(vocab)\n",
        "\n",
        "# перевод множества в список\n",
        "word2index = {}\n",
        "# циклический проход каждой пары элемента и индекса в списке vocab и соответствующему вхождению словаря word2index\n",
        "for i,word in enumerate(vocab):\n",
        "    word2index[word]=i\n",
        "    \n",
        "\n",
        "def words2indices(sentence):\n",
        "    idx = list()\n",
        "    #добавление значения для каждого слова из word2index по соответствующему индексу\n",
        "    for word in sentence:\n",
        "        idx.append(word2index[word])\n",
        "    return idx\n",
        "\n",
        "def softmax(x):\n",
        "  #расчет экспонентных значений всех значений массива x, уменьшенных на смещение np.max(x)\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    #деление каждого значения в массиве на сумму всех значений в массиве ex и возврат нового массива\n",
        "    return e_x / e_x.sum(axis=0)"
      ],
      "metadata": {
        "id": "j5RrSRVZ-Eos"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Задаем сид для генератора псевдослучайных чисел библиотеки NumPy,\n",
        "# чтобы получить одинаковый результат каждый раз при запуске программы\n",
        "np.random.seed(1)\n",
        "#Задаем размерность вектора эмбеддинга слов.\n",
        "embed_size = 10\n",
        "\n",
        "# word embeddings\n",
        "#Создаем матрицу векторов эмбеддинга слов. Используем генератор псевдослучайных чисел библиотеки NumPy,\n",
        "# чтобы заполнить матрицу случайными значениями. Отнимаем 0.5, чтобы значения находились в диапазоне [-0.5, 0.5].\n",
        "#  Затем умножаем на 0.1, чтобы значения находились в диапазоне [-0.05, 0.05].\n",
        "embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1\n",
        "#Создаем единичную матрицу заданной размерности, чтобы использовать её для перехода от одного состояния скрытого слоя к другому.\n",
        "# embedding -> embedding (initially the identity matrix)\n",
        "recurrent = np.eye(embed_size)\n",
        "\n",
        "# sentence embedding for empty sentence\n",
        "#Создаем нулевой вектор размерностью embed_size для получения начального скрытого состояния модели.\n",
        "start = np.zeros(embed_size)\n",
        "\n",
        "# embedding -> output weights\n",
        "decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1\n",
        "#Создаем матрицу весов для перехода от эмбеддинга слов к выходному слою.\n",
        "# Используем генератор псевдослучайных чисел библиотеки NumPy, чтобы заполнить матрицу случайными значениями.\n",
        "# Отнимаем 0.5, чтобы значения находились в диапазоне [-0.5, 0.5].\n",
        "# Затем умножаем на 0.1, чтобы значения находились в диапазоне [-0.05, 0.05].\n",
        "\n",
        "# one hot lookups (for loss function)\n",
        "#Создаем единичную матрицу, чтобы использовать ее для one-hot encoding меток слов в функции потерь.\n",
        "one_hot = np.eye(len(vocab))"
      ],
      "metadata": {
        "id": "mTW0onMVf6UV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прямое распространение с произвольной длиной"
      ],
      "metadata": {
        "id": "Vo1yFjO6f_Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Функция predict(sent) выполняет прогнозы для заданного текста sent.\n",
        "def predict(sent):\n",
        "    #Первые три строчки создают массив слоев neural network. Затем создаются пустой словарь и словарь с ключом 'hidden' и значением \"start\". \n",
        "#'Hidden' - это вектор признаков, который хранит скрытые состояния, которые постепенно вычисляются на каждом шаге.\n",
        "    layers = list()\n",
        "    layer = {}\n",
        "    layer['hidden'] = start\n",
        "   # Затем словарь помещается в список layers.\n",
        "   # Теперь первый элемент списка layers (0) - это входной слой с начальным скрытым состоянием.\n",
        "    layers.append(layer)\n",
        "\n",
        "#Переменная loss инициализируется нулем, она будет использоваться для расчета потерь.\n",
        "    loss = 0\n",
        "\n",
        "#Цикл for выполняет прямое распространение на каждой итерации для каждого элемента текста sent.\n",
        "#Запускается также внутренний цикл for, который по очереди обрабатывает длину каждого элемента текста.\n",
        "    # forward propagate\n",
        "    preds = list()\n",
        " # На каждой итерации словарь создается с ключом \"pred\",\n",
        "  # который хранит значение softmax вектора текущей скрытой нейронной сети, умноженного на матрицу decoder. \n",
        "#Softmax обычно используется для преобразования логита в значения вероятности.\n",
        "# Затем нейронная сеть пытается предсказать следующий термин передавая, скрытое состояние layers-1'hidden',\n",
        "# и матрицу рекурсии recurrent, и добавляя вектор, представляющий текущий термин (embedsent[target_i]).\n",
        "    for target_i in range(len(sent)):\n",
        "\n",
        "        layer = {}\n",
        "\n",
        "        # try to predict the next term\n",
        "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))\n",
        "\n",
        "#Суммирование и запись значения потерь \n",
        "        loss += -np.log(layer['pred'][sent[target_i]])\n",
        "\n",
        "        # generate the next hidden state\n",
        "        #Созданный словарь добавляется в конец списка layers.\n",
        "        layer['hidden'] = layers[-1]['hidden'].dot(recurrent) + embed[sent[target_i]]\n",
        "        layers.append(layer)\n",
        "\n",
        "    return layers, loss"
      ],
      "metadata": {
        "id": "xDg7CvQtgFPF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обратное распространение с произвольной длиной"
      ],
      "metadata": {
        "id": "rP8ndqzGgd1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward\n",
        "for iter in range(30000):\n",
        "    alpha = 0.001#скорость обучения\n",
        "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
        "    layers,loss = predict(sent) #возвращаем слои и потери\n",
        "\n",
        "    # back propagate\n",
        "    #происходит обход слоев в обратном порядке\n",
        "    for layer_idx in reversed(range(len(layers))):\n",
        "#Для слоя определяется индекс (layeridx).\n",
        "#target определяется как индекс предыдущего слова.\n",
        "        layer = layers[layer_idx]\n",
        "        target = sent[layer_idx-1]\n",
        "#Если индекс слоя layeridx больше 0, то движение производится в обратном направлении\n",
        "        if(layer_idx > 0):  # if not the first layer\n",
        "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
        "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
        "\n",
        "            # if the last layer - don't pull from a later one becasue it doesn't exist\n",
        "            ## # если последний слой - не извлекайте из более позднего, потому что он не существует\n",
        "            if(layer_idx == len(layers)-1):\n",
        "                layer['hidden_delta'] = new_hidden_delta\n",
        "            else:\n",
        "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
        "        else: # if the first layer\n",
        "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())"
      ],
      "metadata": {
        "id": "cATGogKXge85"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обновление веса с произвольной длиной"
      ],
      "metadata": {
        "id": "deBcjN_cgqEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward\n",
        "for iter in range(30000):\n",
        "    alpha = 0.001\n",
        "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
        "\n",
        "    layers,loss = predict(sent) \n",
        "\n",
        "    # back propagate\n",
        "    for layer_idx in reversed(range(len(layers))):#цикл, перебирающий все слои нейронной сети в обратном порядке.\n",
        "        layer = layers[layer_idx]#выбор текущего слоя, через который проходит обратное распространение ошибки.\n",
        "        target = sent[layer_idx-1]#выбираем текущее слово, которое передается дальше или далее для предсказания, в зависимости от порядка слоя.\n",
        "\n",
        "        if(layer_idx > 0):\n",
        "          #В данной строке кода определяется, как будет обновляться скрытый слой и его веса. Также определяется,\n",
        "          # как будет обновляться вектор ошибки. Если текущий слой является последним, то вектор ошибки определяется по формуле\n",
        "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
        "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
        "\n",
        "            # if the last layer - don't pull from a \n",
        "            # later one becasue it doesn't exist\n",
        "            if(layer_idx == len(layers)-1):\n",
        "                layer['hidden_delta'] = new_hidden_delta\n",
        "            else:\n",
        "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
        "        else:\n",
        "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(recurrent.transpose())\n",
        "\n",
        "    # update weights\n",
        "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
        "\n",
        "    #В данной строчке определяется, каким будет обновление весов нейронной сети.\n",
        "    # Веса обновляются в соответствие с предыдущим итерационным знанием на основе текущего улучшения. \n",
        "    for layer_idx,layer in enumerate(layers[1:]):\n",
        "        \n",
        "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\n",
        "        \n",
        "        embed_idx = sent[layer_idx]\n",
        "        embed[embed_idx] -= layers[layer_idx]['hidden_delta'] * alpha / float(len(sent))\n",
        "        recurrent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
        "        \n",
        "    if(iter % 1000 == 0):\n",
        "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))\n",
        "        #Вывод perplexity является одним из способов подсчета точности модели, которая была создана,\n",
        "        # с учетом действительности факта, что чем меньше значение perplexity, тем лучше работает нейронная сеть."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fta3hvMrgq_t",
        "outputId": "5e8187fc-9f87-4496-8790-57547909d90e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity:82.02110463450072\n",
            "Perplexity:81.94010350865315\n",
            "Perplexity:81.81574811495292\n",
            "Perplexity:81.56914786505214\n",
            "Perplexity:81.04263651478377\n",
            "Perplexity:79.86348992916052\n",
            "Perplexity:76.91877342505654\n",
            "Perplexity:66.92072862414541\n",
            "Perplexity:37.94708826617198\n",
            "Perplexity:22.07009288920493\n",
            "Perplexity:19.143939179162775\n",
            "Perplexity:18.014630948257548\n",
            "Perplexity:16.762129602575182\n",
            "Perplexity:14.965611373128173\n",
            "Perplexity:12.31726459669751\n",
            "Perplexity:9.241861975008204\n",
            "Perplexity:7.293871890556201\n",
            "Perplexity:6.2664142563626575\n",
            "Perplexity:5.548629535550116\n",
            "Perplexity:5.0863678659138385\n",
            "Perplexity:4.800081270629182\n",
            "Perplexity:4.596557541143939\n",
            "Perplexity:4.454997636216196\n",
            "Perplexity:4.3686831598929246\n",
            "Perplexity:4.3183997021642355\n",
            "Perplexity:4.27035537820718\n",
            "Perplexity:4.208223986060514\n",
            "Perplexity:4.1323599623499865\n",
            "Perplexity:4.049911431563135\n",
            "Perplexity:3.9710411212173917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполнение и анализ выходных данных"
      ],
      "metadata": {
        "id": "9tzEEllLgzlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_index = 4\n",
        "#Вызов функции predict с аргументом words2indices(tokens[sent_index])\n",
        "# и присвоение первого элемента результата функции переменной l.\n",
        "# Второй элемент присваивается временной переменной, которая не будет использоваться далее.\n",
        "l,_ = predict(words2indices(tokens[sent_index]))\n",
        "\n",
        "print(tokens[sent_index])\n",
        "#В цикле для каждого элемента each_layer списка l, \n",
        "#начиная со второго и до предпоследнего, выполняются следующие действия:\n",
        "#a. Создание переменной input, которой присваивается значение элемента списка tokens с индексом i текущего элемента цикла.\n",
        " #   b. Создание переменной true, которой присваивается значение следующего за элементом input элемента списка tokens с индексом i.\n",
        "  #  c. Создание переменной pred, которой присваивается значение из словаря vocab, соответствующее индексу элемента с максимальным значением предсказания каждого слоя в each_layer.\n",
        "  #  d. Печать значения строковой переменной, содержащей текст \"Prev Input:\", затем значение переменной input, в случае необходимости дополненное символами пробела до 12 знаков,\n",
        "  # затем значение \"True:\" и переменной true, дополненное символами пробела до 15 знаков, и наконец значение \"Pred:\" и переменной pred.\n",
        "for i,each_layer in enumerate(l[1:-1]):\n",
        "    input = tokens[sent_index][i]\n",
        "    true = tokens[sent_index][i+1]\n",
        "    pred = vocab[each_layer['pred'].argmax()]\n",
        "    print(\"Prev Input:\" + input + (' ' * (12 - len(input))) +\\\n",
        "          \"True:\" + true + (\" \" * (15 - len(true))) + \"Pred:\" + pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5RPjDe6g0I5",
        "outputId": "d6ea1dd7-4210-4856-bb0a-1f617f5638e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sandra', 'moved', 'to', 'the', 'garden.']\n",
            "Prev Input:sandra      True:moved          Pred:is\n",
            "Prev Input:moved       True:to             Pred:to\n",
            "Prev Input:to          True:the            Pred:the\n",
            "Prev Input:the         True:garden.        Pred:bedroom.\n"
          ]
        }
      ]
    }
  ]
}